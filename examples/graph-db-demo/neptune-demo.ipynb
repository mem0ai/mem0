{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune as Graph Memory\n",
    "\n",
    "In this notebook, we will be connecting using a Amazon Neptune Analytics instance as our memory graph storage for Mem0.\n",
    "\n",
    "The Graph Memory storage persists memories in a graph or relationship form when performing `m.add` memory operations. It then uses vector distance algorithms to find related memories during a `m.search` operation. Relationships are returned in the result, and add context to the memories.\n",
    "\n",
    "Reference: [Vector Similarity using Neptune Analytics](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/vector-similarity.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### 1. Install Mem0 with Graph Memory support \n",
    "\n",
    "To use Mem0 with Graph Memory support (as well as other Amazon services), use pip install:\n",
    "\n",
    "```bash\n",
    "pip install \"mem0ai[graph,extras]\"\n",
    "```\n",
    "\n",
    "This command installs Mem0 along with the necessary dependencies for graph functionality (`graph`) and other Amazon dependencies (`extras`).\n",
    "\n",
    "### 2. Connect to Amazon services\n",
    "\n",
    "For this sample notebook, configure `mem0ai` with [Amazon Neptune Analytics](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html) as the graph store, [Amazon OpenSearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html) as the vector store, and [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) for generating embeddings.\n",
    "\n",
    "Use the following guide for setup details: [Setup AWS Bedrock, AOSS, and Neptune](https://docs.mem0.ai/examples/aws_example#aws-bedrock-and-aoss)\n",
    "\n",
    "Your configuration should look similar to:\n",
    "\n",
    "```python\n",
    "config = {\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"aws_bedrock\",\n",
    "        \"config\": {\n",
    "            \"model\": \"amazon.titan-embed-text-v2:0\"\n",
    "        }\n",
    "    },\n",
    "    \"llm\": {\n",
    "        \"provider\": \"aws_bedrock\",\n",
    "        \"config\": {\n",
    "            \"model\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"opensearch\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"mem0\",\n",
    "            \"host\": \"your-opensearch-domain.us-west-2.es.amazonaws.com\",\n",
    "            \"port\": 443,\n",
    "            \"http_auth\": auth,\n",
    "            \"connection_class\": RequestsHttpConnection,\n",
    "            \"pool_maxsize\": 20,\n",
    "            \"use_ssl\": True,\n",
    "            \"verify_certs\": True,\n",
    "            \"embedding_model_dims\": 1024,\n",
    "        }\n",
    "    },\n",
    "    \"graph_store\": {\n",
    "        \"provider\": \"neptune\",\n",
    "        \"config\": {\n",
    "            \"endpoint\": f\"neptune-graph://my-graph-identifier\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all packages and setup logging"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T08:23:32.221381Z",
     "start_time": "2025-08-28T08:23:31.159942Z"
    }
   },
   "source": [
    "from mem0 import Memory\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import boto3\n",
    "from opensearchpy import RequestsHttpConnection, AWSV4SignerAuth\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# logging.getLogger(\"mem0.graphs.neptune.main\").setLevel(logging.DEBUG)\n",
    "# logging.getLogger(\"mem0.graphs.neptune.base\").setLevel(logging.DEBUG)\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    stream=sys.stdout,  # Explicitly set output to stdout\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Mem0 configuration using:\n",
    "- Amazon Bedrock as the embedder\n",
    "- Amazon Neptune Analytics instance as a graph store\n",
    "- OpenSearch as the vector store"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T08:26:35.169922Z",
     "start_time": "2025-08-28T08:26:35.159539Z"
    }
   },
   "source": [
    "bedrock_embedder_model = \"amazon.titan-embed-text-v2:0\"\n",
    "bedrock_llm_model = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "embedding_model_dims = 1024\n",
    "\n",
    "graph_identifier = os.environ.get(\"GRAPH_ID\")\n",
    "\n",
    "opensearch_host = os.environ.get(\"OS_HOST\")\n",
    "opensearch_port = os.environ.get(\"OS_PORT\")\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "auth = AWSV4SignerAuth(credentials, region)\n",
    "\n",
    "config = {\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"aws_bedrock\",\n",
    "        \"config\": {\n",
    "            \"model\": bedrock_embedder_model,\n",
    "        }\n",
    "    },\n",
    "    \"llm\": {\n",
    "        \"provider\": \"aws_bedrock\",\n",
    "        \"config\": {\n",
    "            \"model\": bedrock_llm_model,\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"opensearch\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"mem0ai_vector_demo\",\n",
    "            \"host\": opensearch_host,\n",
    "            \"port\": opensearch_port,\n",
    "            \"http_auth\": auth,\n",
    "            \"embedding_model_dims\": embedding_model_dims,\n",
    "            \"use_ssl\": True,\n",
    "            \"verify_certs\": True,\n",
    "            \"connection_class\": RequestsHttpConnection,\n",
    "        },\n",
    "    },\n",
    "    \"graph_store\": {\n",
    "        \"provider\": \"neptune\",\n",
    "        \"config\": {\n",
    "            \"endpoint\": f\"neptune-graph://{graph_identifier}\",\n",
    "        },\n",
    "    },\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Memory initializiation\n",
    "\n",
    "Initialize Memgraph as a Graph Memory store:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T08:26:41.386480Z",
     "start_time": "2025-08-28T08:26:37.746137Z"
    }
   },
   "source": [
    "m = Memory.from_config(config_dict=config)\n",
    "\n",
    "user_id = \"Andrew\"\n",
    "\n",
    "m.delete_all(user_id=user_id)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - Creating index mem0ai_vector_demo, it might take 1-2 minutes...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Memories deleted successfully!'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store memories\n",
    "\n",
    "Create memories and store one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T08:29:12.192300Z",
     "start_time": "2025-08-28T08:28:41.699613Z"
    }
   },
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Gremlin is the graph traversal language of Apache TinkerPop. Gremlin is a functional, data-flow language that enables users to succinctly express complex traversals on (or queries of) their application's property graph. Every Gremlin traversal is composed of a sequence of (potentially nested) steps. A step performs an atomic operation on the data stream. Every step is either a map-step (transforming the objects in the stream), a filter-step (removing objects from the stream), or a sideEffect-step (computing statistics about the stream). The Gremlin step library extends on these 3-fundamental operations to provide users a rich collection of steps that they can compose in order to ask any conceivable question they may have of their data for Gremlin is Turing Complete.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Store inferred memories (default behavior)\n",
    "result = m.add(messages, user_id=user_id, metadata={\"category\": \"gremlin\"})\n",
    "\n",
    "all_results = m.get_all(user_id=user_id)\n",
    "print(f\"all_results: {all_results}\")\n",
    "# for n in all_results[\"results\"]:\n",
    "#     print(f\"node \\\"{n['memory']}\\\": [hash: {n['hash']}]\")\n",
    "#\n",
    "# for e in all_results[\"relations\"]:\n",
    "#     print(f\"edge \\\"{e['source']}\\\" --{e['relationship']}--> \\\"{e['target']}\\\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_results: {'results': [], 'relations': [{'source': 'map-step', 'relationship': 'transforms', 'target': 'property_graph'}, {'source': 'map-step', 'relationship': 'is_type_of', 'target': 'step'}, {'source': 'traversal', 'relationship': 'composed_of', 'target': 'step'}, {'source': 'filter-step', 'relationship': 'removes_objects_from', 'target': 'property_graph'}, {'source': 'filter-step', 'relationship': 'is_type_of', 'target': 'step'}, {'source': 'step', 'relationship': 'operates_on', 'target': 'traversal'}, {'source': 'step', 'relationship': 'performs_operation_on', 'target': 'property_graph'}, {'source': 'gremlin', 'relationship': 'is', 'target': 'turing_complete'}, {'source': 'gremlin', 'relationship': 'is_turing_complete', 'target': 'gremlin'}, {'source': 'gremlin', 'relationship': 'queries', 'target': 'property_graph'}, {'source': 'gremlin', 'relationship': 'is', 'target': 'graph_traversal'}, {'source': 'gremlin', 'relationship': 'enables', 'target': 'graph_traversal'}, {'source': 'gremlin', 'relationship': 'part_of', 'target': 'apache_tinkerpop'}, {'source': 'sideeffect-step', 'relationship': 'computes_statistics_about', 'target': 'property_graph'}, {'source': 'sideeffect-step', 'relationship': 'is_type_of', 'target': 'step'}, {'source': 'gremlin_step_library', 'relationship': 'extends', 'target': 'step'}]}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_results = m.get_all(user_id=user_id)\n",
    "print(f\"all_results: {all_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Gremlin is the graph traversal language of Apache TinkerPop. Gremlin is a functional, data-flow language that enables users to succinctly express complex traversals on (or queries of) their application's property graph. Every Gremlin traversal is composed of a sequence of (potentially nested) steps. A step performs an atomic operation on the data stream. Every step is either a map-step (transforming the objects in the stream), a filter-step (removing objects from the stream), or a sideEffect-step (computing statistics about the stream). The Gremlin step library extends on these 3-fundamental operations to provide users a rich collection of steps that they can compose in order to ask any conceivable question they may have of their data for Gremlin is Turing Complete.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"You're absolutely right — that’s a solid summary of Gremlin and how it operates within the Apache TinkerPop graph computing framework.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Gremlin was designed according to the 'write once, run anywhere'-philosophy. This means that not only can all TinkerPop-enabled graph systems execute Gremlin traversals, but also, every Gremlin traversal can be evaluated as either a real-time database query or as a batch analytics query. The former is known as an online transactional process (OLTP) and the latter as an online analytics process (OLAP). This universality is made possible by the Gremlin traversal machine. This distributed, graph-based virtual machine understands how to coordinate the execution of a multi-machine graph traversal. Moreover, not only can the execution either be OLTP or OLAP, it is also possible for certain subsets of a traversal to execute OLTP while others via OLAP. The benefit is that the user does not need to learn both a database query language and a domain-specific BigData analytics language (e.g. Spark DSL, MapReduce, etc.). Gremlin is all that is required to build a graph-based application because the Gremlin traversal machine will handle the rest.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Exactly — what you’ve described here is one of Gremlin’s most powerful and distinctive design features: its platform-agnostic, dual-execution capability enabled by the Gremlin traversal machine.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"A Gremlin traversal can be written in either an imperative (procedural) manner, a declarative (descriptive) manner, or in a hybrid manner containing both imperative and declarative aspects. An imperative Gremlin traversal tells the traversers how to proceed at each step in the traversal. For instance, the imperative traversal in the first box first places a traverser at the vertex denoting Gremlin. That traverser then splits itself across all of Gremlin's collaborators that are not Gremlin himself. Next, the traversers walk to the managers of those collaborators to ultimately be grouped into a manager name count distribution. This traversal is imperative in that it tells the traversers to 'go here and then go there' in an explicit, procedural manner.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"A declarative Gremlin traversal does not tell the traversers the order in which to execute their walk, but instead, allows each traverser to select a pattern to execute from a collection of (potentially nested) patterns. The declarative traversal in the second box yields the same result as the imperative traversal above. However, the declarative traversal has the added benefit that it leverages not only a compile-time query planner (like imperative traversals), but also a runtime query planner that chooses which traversal pattern to execute next based on the historic statistics of each pattern -- favoring those patterns which tend to reduce/filter the most data.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Yes — exactly. Gremlin's flexibility in supporting imperative, declarative, and hybrid styles is one of its key strengths, especially when working with graph-based systems where both traversal logic and expressive querying are important.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The user can write their traversals in any way they choose. However, ultimately when their traversal is compiled, and depending on the underlying execution engine (i.e. an OLTP graph database or an OLAP graph processor), the user's traversal is rewritten by a set of traversal strategies which do their best to determine the most optimal execution plan based on an understanding of graph data access costs as well as the underlying data systems's unique capabilities (e.g. fetch the Gremlin vertex from the graph database's 'name'-index). Gremlin has been designed to give users flexibility in how they express their queries and graph system providers flexibility in how to efficiently evaluate traversals against their TinkerPop-enabled data system.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Exactly — this is a key strength of Gremlin and the TinkerPop architecture.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Store inferred memories (default behavior)\n",
    "result = m.add(messages, user_id=user_id, metadata={\"category\": \"movie_recommendations\"})\n",
    "\n",
    "all_results = m.get_all(user_id=user_id)\n",
    "for n in all_results[\"results\"]:\n",
    "    print(f\"node \\\"{n['memory']}\\\": [hash: {n['hash']}]\")\n",
    "\n",
    "for e in all_results[\"relations\"]:\n",
    "    print(f\"edge \\\"{e['source']}\\\" --{e['relationship']}--> \\\"{e['target']}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Store inferred memories (default behavior)\n",
    "result = m.add(messages, user_id=user_id, metadata={\"category\": \"movie_recommendations\"})\n",
    "\n",
    "all_results = m.get_all(user_id=user_id)\n",
    "for n in all_results[\"results\"]:\n",
    "    print(f\"node \\\"{n['memory']}\\\": [hash: {n['hash']}]\")\n",
    "\n",
    "for e in all_results[\"relations\"]:\n",
    "    print(f\"edge \\\"{e['source']}\\\" --{e['relationship']}--> \\\"{e['target']}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Store inferred memories (default behavior)\n",
    "result = m.add(messages, user_id=user_id, metadata={\"category\": \"movie_recommendations\"})\n",
    "\n",
    "all_results = m.get_all(user_id=user_id)\n",
    "for n in all_results[\"results\"]:\n",
    "    print(f\"node \\\"{n['memory']}\\\": [hash: {n['hash']}]\")\n",
    "\n",
    "for e in all_results[\"relations\"]:\n",
    "    print(f\"edge \\\"{e['source']}\\\" --{e['relationship']}--> \\\"{e['target']}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search memories\n",
    "\n",
    "Search all memories for \"what does alice love?\".  Since \"alice\" the user, this will search for a relationship that fits the users love of \"sci-fi\" movies and dislike of \"thriller\" movies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T08:14:11.188179Z",
     "start_time": "2025-08-28T08:14:07.915115Z"
    }
   },
   "source": [
    "search_results = m.search(\"what does Gremlin do?\", user_id=user_id)\n",
    "for result in search_results[\"results\"]:\n",
    "    print(f\"\\\"{result['memory']}\\\" [score: {result['score']}]\")\n",
    "for relation in search_results[\"relations\"]:\n",
    "    print(f\"{relation}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - _search_graph_db\n",
      "  query=\n",
      "            MATCH (n )\n",
      "            WHERE n.user_id = $user_id\n",
      "            WITH n, $n_embedding as n_embedding\n",
      "            CALL neptune.algo.vectors.distanceByEmbedding(\n",
      "                n_embedding,\n",
      "                n,\n",
      "                {metric:\"CosineSimilarity\"}\n",
      "            ) YIELD distance\n",
      "            WITH n, distance as similarity\n",
      "            WHERE similarity >= $threshold\n",
      "            CALL {\n",
      "                WITH n\n",
      "                MATCH (n)-[r]->(m) \n",
      "                RETURN n.name AS source, id(n) AS source_id, type(r) AS relationship, id(r) AS relation_id, m.name AS destination, id(m) AS destination_id\n",
      "                UNION ALL\n",
      "                WITH n\n",
      "                MATCH (m)-[r]->(n) \n",
      "                RETURN m.name AS source, id(m) AS source_id, type(r) AS relationship, id(r) AS relation_id, n.name AS destination, id(n) AS destination_id\n",
      "            }\n",
      "            WITH distinct source, source_id, relationship, relation_id, destination, destination_id, similarity\n",
      "            RETURN source, source_id, relationship, relation_id, destination, destination_id, similarity\n",
      "            ORDER BY similarity DESC\n",
      "            LIMIT $limit\n",
      "            \n",
      "{'source': 'gremlin_traversal_machine', 'relationship': 'executes', 'destination': 'gremlin'}\n",
      "{'source': 'traversal_strategies', 'relationship': 'optimizes', 'destination': 'gremlin'}\n",
      "{'source': 'andrew', 'relationship': 'confirms', 'destination': 'gremlin'}\n",
      "{'source': 'gremlin', 'relationship': 'supports', 'destination': 'declarative_traversal'}\n",
      "{'source': 'gremlin', 'relationship': 'supports', 'destination': 'imperative_traversal'}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T08:25:28.725574Z",
     "start_time": "2025-08-28T08:25:26.489703Z"
    }
   },
   "source": [
    "m.delete_all(user_id)\n",
    "m.reset()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - Resetting all memories\n",
      "WARNING - Resetting index mem0ai_vector_store...\n",
      "WARNING - Creating index mem0ai_vector_store, it might take 1-2 minutes...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example we demonstrated how an AWS tech stack can be used to store and retrieve memory context. Bedrock LLM models can be used to interpret given conversations.  OpenSearch can store text chunks with vector embeddings. Neptune Analytics can store the text chunks in a graph format with relationship entities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
