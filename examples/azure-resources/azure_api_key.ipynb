{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using **mem0** with Azure resources and API keys\n",
    "\n",
    "This example demonstrates how to utilize **mem0** with API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### 1. Install Mem0\n",
    "\n",
    "To use Mem0, install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install \"mem0ai\"\n",
    "```\n",
    "\n",
    "### 2. Obtain API keys\n",
    "\n",
    "### *Azure OpenAI setup*\n",
    "- In the Azure portal, go to your **Azure OpenAI** resource.\n",
    "- In the left hand channel expand *Resource Management*.\n",
    "- Select *Keys and Endpoint*\n",
    "- Copy either Key 1 or Key 2 to your clipboard and paste in the `your-azure-openai-api-key` constant below.\n",
    "  - If local authentication is disabled for your service you must use an Azure Identity credential, see the help docs and the Azure Identity example for more information.\n",
    "\n",
    "### *Azure Search setup*\n",
    "- In the Azure portal, go to your **Azure AI Search** resource.\n",
    "- In the left hand channel expand *Settings*.\n",
    "- Select *Keys*\n",
    "- Copy either Key 1 or Key 2 to your clipboard and paste in the `your-azure-search-api-key` constant below.\n",
    "  - If *Role-based access control* is selected, no API keys will be available and you must use an Azure Identity credential, see the help docs and the Azure Identity example for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from mem0 import Memory\n",
    "\n",
    "# Change these constants to your Azure OpenAI and Azure AI Search configurations\n",
    "azure_openai_api_key = \"<your-azure-openai-api-key>\"\n",
    "\n",
    "azure_openai_chat_model = \"<your-azure-model>\"  # e.g., \"gpt-4o\n",
    "azure_openai_endpoint = \"<your-azure-endpoint>\"  # e.g., \"https://your-azure-openai-endpoint.openai.azure.com/\"\n",
    "azure_openai_chat_deployment = \"<your-azure-deployment>\"  # e.g., \"gpt-4o\"\n",
    "azure_openai_chat_api_version = \"<your-api-version>\"  # e.g., \"2024-12-01-preview\"\n",
    "\n",
    "azure_openai_embedding_model = \"<your-embedding-model-name>\"  # e.g., \"text-embedding-3-large\"\n",
    "azure_openai_embedding_endpoint = (\n",
    "    \"<your-azure-endpoint>\"  # e.g., \"https://your-azure-openai-endpoint.openai.azure.com/\"\n",
    ")\n",
    "azure_openai_embedding_deployment = \"<your-embedding-deployment>\"  # e.g., \"text-embedding-3-large\"\n",
    "azure_openai_embedding_api_version = \"<your-embedding-api-version>\"  # e.g., \"2024-02-01\"\n",
    "\n",
    "azure_search_api_key = \"<your-azure-search-api-key>\"\n",
    "\n",
    "azure_search_service_name = \"<your-azure-search-service-name>\"  # e.g., \"my-aiss\" (name of the Azure AI Search resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create **mem0** configuration\n",
    "\n",
    "1. LLM uses Azure OpenAI\n",
    "2. Embedder uses Azure OpenAI\n",
    "3. Vector Store uses Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"azure_openai\",\n",
    "        \"config\": {\n",
    "            \"model\": azure_openai_chat_model,\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 2000,\n",
    "            \"azure_kwargs\": {\n",
    "                \"azure_endpoint\": azure_openai_endpoint,\n",
    "                \"azure_deployment\": azure_openai_chat_deployment,\n",
    "                \"api_version\": azure_openai_chat_api_version,\n",
    "                \"api_key\": azure_openai_api_key,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"azure_openai\",\n",
    "        \"config\": {\n",
    "            \"model\": azure_openai_embedding_model,\n",
    "            \"azure_kwargs\": {\n",
    "                \"azure_endpoint\": azure_openai_embedding_endpoint,\n",
    "                \"azure_deployment\": azure_openai_embedding_deployment,\n",
    "                \"api_version\": azure_openai_embedding_api_version,\n",
    "                \"api_key\": azure_openai_api_key,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"azure_ai_search\",\n",
    "        \"config\": {\n",
    "            \"service_name\": azure_search_service_name,\n",
    "            \"collection_name\": \"mem0\",\n",
    "            \"embedding_model_dims\": 3072,\n",
    "            \"api_key\": azure_search_api_key,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Configuration for Azure OpenAI and Azure AI Search.\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Memory.from_config(config_dict=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a memory as a set of chat messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"How about a thriller movies? They can be quite engaging.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\",\n",
    "    },\n",
    "]\n",
    "# Store inferred memories (default behavior)\n",
    "result = m.add(messages, user_id=\"alice\", metadata={\"category\": \"movie_recommendations\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search stored memories\n",
    "for result in m.search(\"what does alice love?\", user_id=\"alice\")[\"results\"]:\n",
    "    print(result[\"memory\"], result[\"score\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_py_3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
