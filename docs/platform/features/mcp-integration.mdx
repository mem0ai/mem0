---
title: MCP Integration
description: "Connect any AI client to Mem0 using Model Context Protocol for universal memory access"
---

> Model Context Protocol (MCP) provides a standardized way for AI agents to manage their own memory through Mem0, without manual API calls.

## Why use MCP

When building AI applications, memory management often requires manual integration. MCP eliminates this complexity by:

- **Universal compatibility**: Works with any MCP-compatible client (Claude Desktop, Cursor, custom agents)
- **Agent autonomy**: AI agents decide when to save, search, or update memories
- **Zero infrastructure**: No servers to maintain - Mem0 handles everything
- **Standardized protocol**: One integration works across all your AI tools

## Available tools

The MCP server exposes 9 memory tools to your AI client:

| Tool | Purpose |
|------|---------|
| `add_memory` | Store conversations or facts |
| `search_memories` | Find relevant memories with filters |
| `get_memories` | List memories with pagination |
| `update_memory` | Modify existing memory content |
| `delete_memory` | Remove specific memories |
| `delete_all_memories` | Bulk delete memories |
| `delete_entities` | Remove user/agent/app entities |
| `get_memory` | Retrieve single memory by ID |
| `list_entities` | View stored entities |

## Deployment options

Choose the deployment method that fits your workflow:

<AccordionGroup>
  <Accordion title="Python package (recommended)">
    Install and run locally with uvx:

    ```bash
    uv pip install mem0-mcp-server
    ```

    Configure your client:
    ```json
    {
      "mcpServers": {
        "mem0": {
          "command": "uvx",
          "args": ["mem0-mcp-server"],
          "env": {
            "MEM0_API_KEY": "m0-...",
            "MEM0_DEFAULT_USER_ID": "your-handle"
          }
        }
      }
    }
    ```
  </Accordion>

  <Accordion title="Docker container">
    Containerized deployment with HTTP endpoint:

    ```bash
    docker build -t mem0-mcp-server https://github.com/mem0ai/mem0-mcp.git
    docker run --rm -d -e MEM0_API_KEY="m0-..." -p 8080:8081 mem0-mcp-server
    ```

    Configure for HTTP:
    ```json
    {
      "mcpServers": {
        "mem0-docker": {
          "command": "curl",
          "args": ["-X", "POST", "http://localhost:8080/mcp", "--data-binary", "@"],
          "env": {
            "MEM0_API_KEY": "m0-..."
          }
        }
      }
    }
    ```
  </Accordion>

  <Accordion title="Smithery">
    One-click setup with managed service:

    Visit [smithery.ai/server/@mem0ai/mem0-memory-mcp](https://smithery.ai/server/@mem0ai/mem0-memory-mcp) and:

    1. Select your AI client (Cursor, Claude Desktop, etc.)
    2. Configure your Mem0 API key
    3. Set your default user ID
    4. Enable graph memory (optional)
    5. Copy the generated configuration

    Your client connects automatically - no installation required.
  </Accordion>
</AccordionGroup>

## Configuration

### Required environment variables
```bash
MEM0_API_KEY="m0-..."                    # Your Mem0 API key
MEM0_DEFAULT_USER_ID="your-handle"        # Default user ID
```

### Optional variables
```bash
MEM0_ENABLE_GRAPH_DEFAULT="true"          # Enable graph memories
MEM0_MCP_AGENT_MODEL="gpt-4o-mini"        # LLM for bundled examples
```

## Example interactions

Once connected, your AI agent can:

```
User: Remember that I'm allergic to peanuts
Agent: [calls add_memory] Got it! I've saved your peanut allergy.

User: What dietary restrictions do I know about?
Agent: [calls search_memories] You have a peanut allergy.
```

The agent automatically decides when to use memory tools based on context.

## Try these prompts

```python
# Multi-task operations
"Generate 5 user personas for our e-commerce app with different demographics, store them all, then search for existing personas"

# Natural context retrieval
"Anything about my work preferences I should remember?"

# Complex information updates
"Update my current project: the mobile app is now 80% complete, we've fixed the login issues, and the launch date is March 15"

# Time-based queries
"What meetings did I have last week about Project Phoenix?"

# Memory cleanup
"Delete all test data and temporary memories from our development phase"

# Personal preferences
"I drink oat milk cappuccino with one sugar every morning, and I prefer standing desks"

# Health and wellness tracking
"I'm allergic to peanuts and shellfish, and I go for 5km runs on weekday mornings"
```

These examples demonstrate how MCP enables natural language memory operations - the AI agent automatically determines when to add, search, update, or delete memories based on context.

## Performance tips

- Enable graph memories for relationship-aware recall
- Use specific filters when searching large memory sets
- Batch operations when adding multiple memories
- Monitor memory usage in the Mem0 dashboard

## Best practices

- **Start simple**: Use the Python package for development
- **Use wildcards**: `user_id: "*"` to search across all users
- **Test locally**: Use the bundled Python agent to verify setup
- **Monitor usage**: Track memory operations in the dashboard
- **Document patterns**: Share successful prompt patterns with your team

{/* DEBUG: verify CTA targets */}

<CardGroup cols={2}>
  <Card
    title="Memory Filters"
    description="Refine memory retrieval with powerful filtering capabilities"
    icon="scale-balanced"
    href="/platform/features/v2-memory-filters"
  />
  <Card
    title="Gemini 3 with MCP"
    description="See MCP in action with Google's Gemini 3 model"
    icon="book-open"
    href="/cookbooks/frameworks/gemini-3-with-mem0-mcp"
  />
</CardGroup>