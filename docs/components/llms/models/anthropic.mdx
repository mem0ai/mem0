---
title: Anthropic
---

<Snippet file="paper-release.mdx" />

To use anthropic's models, please set the `ANTHROPIC_API_KEY` which you find on their [Account Settings Page](https://console.anthropic.com/account/keys).

## Usage

<CodeGroup>
```python Python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "your-api-key" # used for embedding model
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"

config = {
    "llm": {
        "provider": "anthropic",
        "config": {
            "model": "claude-3-7-sonnet-latest",
            "temperature": 0.1,
            "max_tokens": 2000,
        }
    }
}

m = Memory.from_config(config)
messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
m.add(messages, user_id="alice", metadata={"category": "movies"})
```

```typescript TypeScript
import { Memory } from 'mem0ai/oss';

const config = {
  llm: {
    provider: 'anthropic',
    config: {
      apiKey: process.env.ANTHROPIC_API_KEY || '',
      model: 'claude-3-7-sonnet-latest',
      temperature: 0.1,
      maxTokens: 2000,
    },
  },
};

const memory = new Memory(config);
const messages = [
    {"role": "user", "content": "I'm planning to watch a movie tonight. Any recommendations?"},
    {"role": "assistant", "content": "How about a thriller movies? They can be quite engaging."},
    {"role": "user", "content": "I’m not a big fan of thriller movies but I love sci-fi movies."},
    {"role": "assistant", "content": "Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future."}
]
await memory.add(messages, { userId: "alice", metadata: { category: "movies" } });
```
</CodeGroup>

## Config

The `AnthropicLlm` class in Embedchain allows you to configure various parameters for interacting with Anthropic models. These settings are passed from the `llm.config` object in your Embedchain configuration to the underlying `ChatAnthropic` client from Langchain.

Here are the key configuration options available:

-   `model`: (string, required) Specifies the Anthropic model you wish to use. For example: `"claude-3-7-sonnet-latest"`, `"claude-2"`, `"claude-instant-1"`.
-   `temperature`: (float, optional) Controls the randomness of the model's output. Lower values (e.g., 0.1) make the output more deterministic and focused, while higher values make it more creative.
-   `max_tokens`: (integer, optional) The maximum number of tokens the model should generate in its response. This parameter is now correctly passed to the Anthropic API.
-   `top_p`: (float, optional) Nucleus sampling parameter. It controls the cumulative probability of the next token choices. For example, a `top_p` of 0.9 means the model considers tokens comprising the top 90% probability mass.
-   `base_url`: (string, optional) If you need to use a custom or proxy URL for the Anthropic API, you can specify it here. This value is passed as the `anthropic_api_url` to the `ChatAnthropic` client.
-   `model_kwargs`: (dict, optional) A dictionary for any additional keyword arguments you want to pass directly to the Anthropic model. This is useful for accessing newer or more advanced Anthropic parameters that might not have direct equivalents in Embedchain's `BaseLlmConfig`.
-   `callbacks`: (list, optional) A list of Langchain callback handlers. These can be used for logging, streaming, or other custom behaviors during the LLM interaction.
-   `default_headers`: (dict, optional) Allows you to specify custom headers that will be included in every API request made to Anthropic.
-   `http_client`: (Any, optional) You can provide a custom HTTP client instance (e.g., an `httpx.Client`) for making API calls. This is useful for advanced scenarios like custom SSL configurations or transport settings.

While these are the primary options for Anthropic, you can find a [Master List of All Params in Config](../config) for `BaseLlmConfig`. The example above demonstrates basic usage of `model`, `temperature`, and `max_tokens`. You can extend it with other parameters as needed.